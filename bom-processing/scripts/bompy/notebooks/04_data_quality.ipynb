{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bills of Mortality - Data Quality Assessment\n",
    "\n",
    "This notebook provides a comprehensive assessment of data quality in the Bills of Mortality dataset, including:\n",
    "- Missing data patterns\n",
    "- Data consistency checks\n",
    "- Outlier detection\n",
    "- Historical plausibility assessment\n",
    "- Recommendations for data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Bills of Mortality - Data Quality Assessment\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = Path('../data')\n",
    "\n",
    "print(\"üîç Bills of Mortality - Data Quality Assessment\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Examine Raw Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded bills: 1,292,566 records\n",
      "‚úì Loaded parishes: 156 records\n",
      "‚úì Loaded years: 113 records\n",
      "‚úì Loaded weeks: 5,393 records\n",
      "\n",
      "üìä Primary Dataset Info:\n",
      "Shape: (1292566, 10)\n",
      "Columns: ['parish_id', 'count_type', 'count', 'year', 'week_id', 'bill_type', 'missing', 'illegible', 'source', 'unique_identifier']\n",
      "Memory usage: 404.95 MB\n",
      "\n",
      "üìã Data Types:\n",
      "  parish_id            int64      -    149 unique,      0 nulls\n",
      "  count_type           object     -      2 unique,      0 nulls\n",
      "  count                int64      -    310 unique,      0 nulls\n",
      "  year                 int64      -    110 unique,      0 nulls\n",
      "  week_id              object     -  5,005 unique,      0 nulls\n",
      "  bill_type            object     -      2 unique,      0 nulls\n",
      "  missing              bool       -      2 unique,      0 nulls\n",
      "  illegible            bool       -      1 unique,      0 nulls\n",
      "  source               object     -      7 unique,      0 nulls\n",
      "  unique_identifier    object     -  5,299 unique,      0 nulls\n"
     ]
    }
   ],
   "source": [
    "# Load all datasets\n",
    "datasets = {}\n",
    "file_paths = {\n",
    "    'bills': 'all_bills.csv',\n",
    "    'parishes': 'parishes.csv',\n",
    "    'years': 'years.csv',\n",
    "    'weeks': 'weeks.csv'\n",
    "}\n",
    "\n",
    "for name, filename in file_paths.items():\n",
    "    filepath = DATA_DIR / filename\n",
    "    if filepath.exists():\n",
    "        datasets[name] = pd.read_csv(filepath)\n",
    "        print(f\"‚úì Loaded {name}: {len(datasets[name]):,} records\")\n",
    "    else:\n",
    "        print(f\"‚úó Missing file: {filepath}\")\n",
    "\n",
    "# Check if we have the main dataset\n",
    "if 'bills' not in datasets:\n",
    "    print(\"‚ùå Cannot proceed without main bills dataset\")\n",
    "    exit()\n",
    "\n",
    "bills = datasets['bills']\n",
    "parishes = datasets.get('parishes')\n",
    "years = datasets.get('years')\n",
    "weeks = datasets.get('weeks')\n",
    "\n",
    "print(f\"\\nüìä Primary Dataset Info:\")\n",
    "print(f\"Shape: {bills.shape}\")\n",
    "print(f\"Columns: {list(bills.columns)}\")\n",
    "print(f\"Memory usage: {bills.memory_usage(deep=True).sum() / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Data types\n",
    "print(f\"\\nüìã Data Types:\")\n",
    "for col, dtype in bills.dtypes.items():\n",
    "    unique_count = bills[col].nunique()\n",
    "    null_count = bills[col].isnull().sum()\n",
    "    print(f\"  {col:20} {str(dtype):10} - {unique_count:>6,} unique, {null_count:>6,} nulls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive missing data assessment\n",
    "missing_summary = []\n",
    "\n",
    "for col in bills.columns:\n",
    "    null_count = bills[col].isnull().sum()\n",
    "    null_pct = (null_count / len(bills)) * 100\n",
    "    missing_summary.append({\n",
    "        'column': col,\n",
    "        'null_count': null_count,\n",
    "        'null_percentage': null_pct,\n",
    "        'data_type': str(bills[col].dtype)\n",
    "    })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_summary).sort_values('null_percentage', ascending=False)\n",
    "\n",
    "print(f\"üï≥Ô∏è Missing Data Summary:\")\n",
    "print(\"=\"*45)\n",
    "for _, row in missing_df.iterrows():\n",
    "    if row['null_count'] > 0:\n",
    "        print(f\"  {row['column']:20} - {row['null_count']:>6,} missing ({row['null_percentage']:>5.1f}%)\")\n",
    "\n",
    "if missing_df['null_count'].sum() == 0:\n",
    "    print(\"  ‚úÖ No missing values detected in any column\")\n",
    "\n",
    "# Visualize missing data patterns\n",
    "if missing_df['null_count'].sum() > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Missing data heatmap\n",
    "    missing_data = bills.isnull()\n",
    "    sns.heatmap(missing_data.T, cbar=True, ax=axes[0], cmap='viridis')\n",
    "    axes[0].set_title('Missing Data Pattern (Dark = Missing)')\n",
    "    axes[0].set_xlabel('Records (sample)')\n",
    "    \n",
    "    # Missing data bar chart\n",
    "    missing_cols = missing_df[missing_df['null_count'] > 0]\n",
    "    if not missing_cols.empty:\n",
    "        axes[1].barh(missing_cols['column'], missing_cols['null_percentage'], color='red', alpha=0.7)\n",
    "        axes[1].set_xlabel('Percentage Missing')\n",
    "        axes[1].set_title('Missing Data by Column')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Check for patterns in missing data\n",
    "print(f\"\\nüîç Missing Data Patterns:\")\n",
    "if 'year' in bills.columns:\n",
    "    missing_by_year = bills.groupby('year').apply(lambda x: x.isnull().sum().sum()).sort_values(ascending=False)\n",
    "    if missing_by_year.sum() > 0:\n",
    "        print(f\"Years with most missing data:\")\n",
    "        for year, missing_count in missing_by_year.head(5).items():\n",
    "            if missing_count > 0:\n",
    "                print(f\"  {year}: {missing_count} missing values\")\n",
    "    else:\n",
    "        print(f\"  No missing data patterns by year\")\n",
    "\n",
    "if 'parish_id' in bills.columns and missing_df['null_count'].sum() > 0:\n",
    "    missing_by_parish = bills.groupby('parish_id').apply(lambda x: x.isnull().sum().sum()).sort_values(ascending=False)\n",
    "    if missing_by_parish.sum() > 0:\n",
    "        print(f\"\\nParishes with most missing data:\")\n",
    "        for parish_id, missing_count in missing_by_parish.head(5).items():\n",
    "            if missing_count > 0:\n",
    "                parish_name = parishes.set_index('id')['parish_name'].get(parish_id, f'Parish {parish_id}') if parishes is not None else f'Parish {parish_id}'\n",
    "                print(f\"  {parish_name[:30]:30} - {missing_count} missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Consistency Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consistency_issues = []\n",
    "\n",
    "print(f\"üîÑ Data Consistency Checks:\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# 1. Check for negative death counts\n",
    "if 'count' in bills.columns:\n",
    "    negative_counts = bills[bills['count'] < 0]\n",
    "    if len(negative_counts) > 0:\n",
    "        print(f\"‚ùå Negative death counts: {len(negative_counts)} records\")\n",
    "        consistency_issues.append(f\"Negative death counts: {len(negative_counts)} records\")\n",
    "    else:\n",
    "        print(f\"‚úÖ No negative death counts\")\n",
    "\n",
    "# 2. Check for unreasonably high death counts\n",
    "if 'count' in bills.columns:\n",
    "    q99 = bills['count'].quantile(0.99)\n",
    "    extreme_counts = bills[bills['count'] > q99 * 3]  # 3x the 99th percentile\n",
    "    if len(extreme_counts) > 0:\n",
    "        max_count = bills['count'].max()\n",
    "        print(f\"‚ö†Ô∏è Extremely high death counts: {len(extreme_counts)} records (max: {max_count})\")\n",
    "        consistency_issues.append(f\"Extremely high death counts: {len(extreme_counts)} records\")\n",
    "        \n",
    "        # Show the highest counts\n",
    "        top_counts = bills.nlargest(5, 'count')[['parish_id', 'year', 'week_id', 'count', 'count_type']]\n",
    "        print(f\"  Top 5 highest counts:\")\n",
    "        for _, record in top_counts.iterrows():\n",
    "            parish_name = parishes.set_index('id')['parish_name'].get(record['parish_id'], f'Parish {record[\"parish_id\"]}') if parishes is not None else f'Parish {record[\"parish_id\"]}'\n",
    "            print(f\"    {parish_name[:25]:25} {record['year']} - {record['count']} deaths\")\n",
    "    else:\n",
    "        print(f\"‚úÖ No extremely high death counts\")\n",
    "\n",
    "# 3. Check year ranges\n",
    "if 'year' in bills.columns:\n",
    "    min_year, max_year = bills['year'].min(), bills['year'].max()\n",
    "    print(f\"üìÖ Year range: {min_year} - {max_year}\")\n",
    "    \n",
    "    # Check for implausible years\n",
    "    implausible_years = bills[(bills['year'] < 1500) | (bills['year'] > 1800)]\n",
    "    if len(implausible_years) > 0:\n",
    "        print(f\"‚ùå Implausible years: {len(implausible_years)} records\")\n",
    "        consistency_issues.append(f\"Implausible years: {len(implausible_years)} records\")\n",
    "    else:\n",
    "        print(f\"‚úÖ All years within plausible range\")\n",
    "\n",
    "# 4. Check parish ID consistency\n",
    "if parishes is not None and 'parish_id' in bills.columns:\n",
    "    bill_parishes = set(bills['parish_id'].unique())\n",
    "    parish_table_ids = set(parishes['id'].unique())\n",
    "    \n",
    "    missing_parish_refs = bill_parishes - parish_table_ids\n",
    "    unused_parishes = parish_table_ids - bill_parishes\n",
    "    \n",
    "    if missing_parish_refs:\n",
    "        print(f\"‚ùå Bills reference {len(missing_parish_refs)} undefined parishes: {list(missing_parish_refs)[:5]}\")\n",
    "        consistency_issues.append(f\"Undefined parish references: {len(missing_parish_refs)}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ All bill parish references are valid\")\n",
    "    \n",
    "    if unused_parishes:\n",
    "        print(f\"‚ÑπÔ∏è {len(unused_parishes)} parishes in lookup table have no bills\")\n",
    "\n",
    "# 5. Check week ID format consistency\n",
    "if 'week_id' in bills.columns:\n",
    "    week_id_sample = bills['week_id'].dropna().head(1000)\n",
    "    \n",
    "    # Check for consistent format (assuming YYYY-YYYY-WW pattern)\n",
    "    pattern = r'^\\d{4}-\\d{4}-\\d{1,2}$'\n",
    "    valid_format = week_id_sample.str.match(pattern, na=False)\n",
    "    invalid_format_count = (~valid_format).sum()\n",
    "    \n",
    "    if invalid_format_count > 0:\n",
    "        print(f\"‚ö†Ô∏è Week IDs with inconsistent format: ~{invalid_format_count * len(bills) // len(week_id_sample)} records\")\n",
    "        consistency_issues.append(f\"Inconsistent week ID format: ~{invalid_format_count * len(bills) // len(week_id_sample)} records\")\n",
    "        \n",
    "        # Show examples of invalid formats\n",
    "        invalid_examples = week_id_sample[~valid_format].head(5).tolist()\n",
    "        print(f\"  Examples: {invalid_examples}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Week ID format is consistent\")\n",
    "\n",
    "# 6. Check for duplicate records\n",
    "key_columns = ['parish_id', 'year', 'week_id', 'count_type']\n",
    "if all(col in bills.columns for col in key_columns):\n",
    "    duplicates = bills.duplicated(subset=key_columns, keep=False)\n",
    "    duplicate_count = duplicates.sum()\n",
    "    \n",
    "    if duplicate_count > 0:\n",
    "        print(f\"‚ùå Potential duplicate records: {duplicate_count}\")\n",
    "        consistency_issues.append(f\"Duplicate records: {duplicate_count}\")\n",
    "        \n",
    "        # Show examples\n",
    "        duplicate_examples = bills[duplicates].head(10)[key_columns + ['count']]\n",
    "        print(f\"  Examples:\")\n",
    "        print(duplicate_examples.to_string(index=False))\n",
    "    else:\n",
    "        print(f\"‚úÖ No duplicate records found\")\n",
    "\n",
    "# Summary of consistency issues\n",
    "print(f\"\\nüìã Consistency Issues Summary:\")\n",
    "if consistency_issues:\n",
    "    for i, issue in enumerate(consistency_issues, 1):\n",
    "        print(f\"  {i}. {issue}\")\n",
    "else:\n",
    "    print(f\"  ‚úÖ No major consistency issues detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìä Statistical Outlier Detection:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if 'count' in bills.columns:\n",
    "    # Basic statistics\n",
    "    count_stats = bills['count'].describe()\n",
    "    print(f\"Death Count Statistics:\")\n",
    "    print(count_stats)\n",
    "    \n",
    "    # IQR method for outlier detection\n",
    "    Q1 = bills['count'].quantile(0.25)\n",
    "    Q3 = bills['count'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers_iqr = bills[(bills['count'] < lower_bound) | (bills['count'] > upper_bound)]\n",
    "    \n",
    "    print(f\"\\nüìà IQR Outlier Detection:\")\n",
    "    print(f\"  IQR bounds: [{lower_bound:.1f}, {upper_bound:.1f}]\")\n",
    "    print(f\"  Outliers detected: {len(outliers_iqr):,} records ({len(outliers_iqr)/len(bills)*100:.2f}%)\")\n",
    "    \n",
    "    # Z-score method\n",
    "    z_scores = np.abs(stats.zscore(bills['count']))\n",
    "    outliers_zscore = bills[z_scores > 3]\n",
    "    \n",
    "    print(f\"\\nüìä Z-Score Outlier Detection (|z| > 3):\")\n",
    "    print(f\"  Outliers detected: {len(outliers_zscore):,} records ({len(outliers_zscore)/len(bills)*100:.2f}%)\")\n",
    "    \n",
    "    # Visualize outliers\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Death Count Outlier Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Box plot\n",
    "    axes[0,0].boxplot(bills['count'])\n",
    "    axes[0,0].set_title('Box Plot of Death Counts')\n",
    "    axes[0,0].set_ylabel('Death Count')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Histogram with outlier boundaries\n",
    "    axes[0,1].hist(bills['count'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0,1].axvline(upper_bound, color='red', linestyle='--', label=f'IQR Upper Bound ({upper_bound:.1f})')\n",
    "    if lower_bound > 0:\n",
    "        axes[0,1].axvline(lower_bound, color='red', linestyle='--', label=f'IQR Lower Bound ({lower_bound:.1f})')\n",
    "    axes[0,1].set_xlabel('Death Count')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    axes[0,1].set_title('Distribution with IQR Bounds')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].set_yscale('log')\n",
    "    \n",
    "    # Q-Q plot\n",
    "    stats.probplot(bills['count'], dist=\"norm\", plot=axes[1,0])\n",
    "    axes[1,0].set_title('Q-Q Plot (Normal Distribution)')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Time series of outliers\n",
    "    if 'year' in bills.columns:\n",
    "        outlier_by_year = outliers_iqr.groupby('year').size()\n",
    "        all_years = bills.groupby('year').size()\n",
    "        outlier_pct_by_year = (outlier_by_year / all_years * 100).fillna(0)\n",
    "        \n",
    "        axes[1,1].plot(outlier_pct_by_year.index, outlier_pct_by_year.values, \n",
    "                       marker='o', linewidth=2, color='red')\n",
    "        axes[1,1].set_xlabel('Year')\n",
    "        axes[1,1].set_ylabel('Percentage of Records')\n",
    "        axes[1,1].set_title('Outliers by Year (%)')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze outlier characteristics\n",
    "    if len(outliers_iqr) > 0:\n",
    "        print(f\"\\nüîç Outlier Characteristics:\")\n",
    "        \n",
    "        # Top outliers\n",
    "        top_outliers = outliers_iqr.nlargest(10, 'count')\n",
    "        print(f\"\\nTop 10 Outliers:\")\n",
    "        for _, outlier in top_outliers.iterrows():\n",
    "            parish_name = parishes.set_index('id')['parish_name'].get(outlier['parish_id'], f'Parish {outlier[\"parish_id\"]}') if parishes is not None else f'Parish {outlier[\"parish_id\"]}'\n",
    "            print(f\"  {parish_name[:25]:25} {outlier['year']} Week {outlier['week_id']:15} - {outlier['count']:,} {outlier.get('count_type', 'deaths')}\")\n",
    "        \n",
    "        # Outliers by year\n",
    "        if 'year' in outliers_iqr.columns:\n",
    "            outlier_years = outliers_iqr['year'].value_counts().head(5)\n",
    "            print(f\"\\nYears with most outliers:\")\n",
    "            for year, count in outlier_years.items():\n",
    "                print(f\"  {year}: {count} outlier records\")\n",
    "        \n",
    "        # Outliers by parish\n",
    "        if 'parish_id' in outliers_iqr.columns:\n",
    "            outlier_parishes = outliers_iqr['parish_id'].value_counts().head(5)\n",
    "            print(f\"\\nParishes with most outliers:\")\n",
    "            for parish_id, count in outlier_parishes.items():\n",
    "                parish_name = parishes.set_index('id')['parish_name'].get(parish_id, f'Parish {parish_id}') if parishes is not None else f'Parish {parish_id}'\n",
    "                print(f\"  {parish_name[:30]:30} - {count} outlier records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Plausibility Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üèõÔ∏è Historical Plausibility Assessment:\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "plausibility_issues = []\n",
    "\n",
    "# 1. Weekly death count plausibility\n",
    "if 'count' in bills.columns:\n",
    "    # For historical London parishes, weekly deaths typically:\n",
    "    # - Small parishes: 0-5 deaths per week normally\n",
    "    # - Large parishes: 5-20 deaths per week normally  \n",
    "    # - Crisis periods: could be 5-10x normal\n",
    "    \n",
    "    very_high_weekly = bills[bills['count'] > 100]  # More than 100 deaths per week\n",
    "    extremely_high = bills[bills['count'] > 500]   # More than 500 deaths per week\n",
    "    \n",
    "    print(f\"Weekly Death Count Plausibility:\")\n",
    "    print(f\"  Records with >100 deaths/week: {len(very_high_weekly):,} ({len(very_high_weekly)/len(bills)*100:.2f}%)\")\n",
    "    print(f\"  Records with >500 deaths/week: {len(extremely_high):,} ({len(extremely_high)/len(bills)*100:.2f}%)\")\n",
    "    \n",
    "    if len(extremely_high) > 0:\n",
    "        print(f\"  ‚ö†Ô∏è Extremely high weekly deaths may indicate:\")\n",
    "        print(f\"     - Major plague outbreaks\")\n",
    "        print(f\"     - Data entry errors\")\n",
    "        print(f\"     - Aggregated data (multiple weeks/parishes)\")\n",
    "        plausibility_issues.append(f\"Extremely high weekly deaths: {len(extremely_high)} records\")\n",
    "\n",
    "# 2. Temporal continuity assessment\n",
    "if 'year' in bills.columns:\n",
    "    year_coverage = bills['year'].value_counts().sort_index()\n",
    "    year_range = range(bills['year'].min(), bills['year'].max() + 1)\n",
    "    missing_years = [year for year in year_range if year not in year_coverage.index]\n",
    "    \n",
    "    print(f\"\\nTemporal Coverage:\")\n",
    "    print(f\"  Year range: {bills['year'].min()} - {bills['year'].max()} ({bills['year'].max() - bills['year'].min() + 1} years)\")\n",
    "    print(f\"  Years with data: {len(year_coverage)}\")\n",
    "    print(f\"  Missing years: {len(missing_years)}\")\n",
    "    \n",
    "    if missing_years:\n",
    "        print(f\"  Missing year examples: {missing_years[:10]}\")\n",
    "        \n",
    "        # Check for long gaps\n",
    "        if len(missing_years) > 5:\n",
    "            plausibility_issues.append(f\"Temporal gaps: {len(missing_years)} missing years\")\n",
    "    \n",
    "    # Check for years with very few records (possible incomplete data)\n",
    "    sparse_years = year_coverage[year_coverage < year_coverage.quantile(0.1)]\n",
    "    if len(sparse_years) > 0:\n",
    "        print(f\"  Years with few records (<10th percentile): {len(sparse_years)}\")\n",
    "        print(f\"  Examples: {dict(sparse_years.head())}\")\n",
    "\n",
    "# 3. Parish activity patterns\n",
    "if 'parish_id' in bills.columns:\n",
    "    parish_activity = bills.groupby('parish_id').agg({\n",
    "        'year': ['min', 'max', 'nunique'],\n",
    "        'count': ['sum', 'count']\n",
    "    }).round(2)\n",
    "    \n",
    "    parish_activity.columns = ['_'.join(col).strip() for col in parish_activity.columns]\n",
    "    parish_activity['years_span'] = parish_activity['year_max'] - parish_activity['year_min'] + 1\n",
    "    parish_activity['years_active'] = parish_activity['year_nunique']\n",
    "    parish_activity['activity_rate'] = parish_activity['years_active'] / parish_activity['years_span']\n",
    "    \n",
    "    print(f\"\\nParish Activity Patterns:\")\n",
    "    print(f\"  Parishes with data: {len(parish_activity)}\")\n",
    "    print(f\"  Average activity rate: {parish_activity['activity_rate'].mean():.2f} (1.0 = continuous)\")\n",
    "    \n",
    "    # Find parishes with very low activity rates (possible data quality issues)\n",
    "    low_activity = parish_activity[parish_activity['activity_rate'] < 0.3]  # Active <30% of their span\n",
    "    if len(low_activity) > 0:\n",
    "        print(f\"  Parishes with low activity (<30% of time span): {len(low_activity)}\")\n",
    "        plausibility_issues.append(f\"Low parish activity: {len(low_activity)} parishes\")\n",
    "\n",
    "# 4. Known historical events cross-check\n",
    "print(f\"\\nHistorical Context Checks:\")\n",
    "\n",
    "# Major plague years in London\n",
    "known_plague_years = [1603, 1625, 1636, 1665, 1666]\n",
    "available_plague_years = [year for year in known_plague_years if year in bills['year'].values]\n",
    "\n",
    "if available_plague_years:\n",
    "    print(f\"  Known plague years in dataset: {available_plague_years}\")\n",
    "    \n",
    "    # Check if mortality is elevated in these years\n",
    "    yearly_mortality = bills.groupby('year')['count'].sum()\n",
    "    normal_mortality = yearly_mortality.drop(available_plague_years, errors='ignore').median()\n",
    "    \n",
    "    for plague_year in available_plague_years:\n",
    "        if plague_year in yearly_mortality.index:\n",
    "            plague_mortality = yearly_mortality[plague_year]\n",
    "            ratio = plague_mortality / normal_mortality if normal_mortality > 0 else 0\n",
    "            print(f\"    {plague_year}: {plague_mortality:,} deaths ({ratio:.1f}x normal)\")\n",
    "            \n",
    "            if ratio < 2:  # Expect at least 2x mortality in plague years\n",
    "                plausibility_issues.append(f\"Low plague year mortality in {plague_year}: only {ratio:.1f}x normal\")\n",
    "else:\n",
    "    print(f\"  No known plague years (1603, 1625, 1636, 1665-1666) in dataset\")\n",
    "\n",
    "# 5. Seasonal pattern plausibility\n",
    "if 'week_id' in bills.columns and bills['week_id'].str.contains('-').any():\n",
    "    # Extract week numbers\n",
    "    week_parts = bills['week_id'].str.split('-', expand=True)\n",
    "    if len(week_parts.columns) >= 3:\n",
    "        bills_temp = bills.copy()\n",
    "        bills_temp['week_num'] = pd.to_numeric(week_parts.iloc[:, -1], errors='coerce')\n",
    "        \n",
    "        # Group by seasons (approximate)\n",
    "        bills_temp['season'] = bills_temp['week_num'].apply(lambda x: \n",
    "            'Winter' if x in [1,2,3,4,5,6,50,51,52] \n",
    "            else 'Spring' if x in [7,8,9,10,11,12,13,14,15,16,17,18,19,20] \n",
    "            else 'Summer' if x in [21,22,23,24,25,26,27,28,29,30,31,32,33,34] \n",
    "            else 'Autumn' if x in [35,36,37,38,39,40,41,42,43,44,45,46,47,48,49] \n",
    "            else 'Unknown')\n",
    "        \n",
    "        seasonal_mortality = bills_temp.groupby('season')['count'].mean()\n",
    "        if 'Winter' in seasonal_mortality.index and 'Summer' in seasonal_mortality.index:\n",
    "            winter_summer_ratio = seasonal_mortality['Winter'] / seasonal_mortality['Summer']\n",
    "            print(f\"\\nSeasonal Mortality Pattern:\")\n",
    "            print(f\"  Winter/Summer mortality ratio: {winter_summer_ratio:.2f}\")\n",
    "            \n",
    "            # Historically, winter mortality was typically higher\n",
    "            if winter_summer_ratio < 0.8:\n",
    "                print(f\"  ‚ö†Ô∏è Unusually low winter mortality (expected higher than summer)\")\n",
    "                plausibility_issues.append(f\"Unusual seasonal pattern: winter {winter_summer_ratio:.2f}x summer\")\n",
    "            else:\n",
    "                print(f\"  ‚úÖ Plausible seasonal pattern\")\n",
    "\n",
    "# Summary of plausibility issues\n",
    "print(f\"\\nüìã Historical Plausibility Issues:\")\n",
    "if plausibility_issues:\n",
    "    for i, issue in enumerate(plausibility_issues, 1):\n",
    "        print(f\"  {i}. {issue}\")\n",
    "else:\n",
    "    print(f\"  ‚úÖ No major historical plausibility issues detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Completeness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìä Data Completeness Analysis:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Overall completeness score\n",
    "total_possible_cells = len(bills) * len(bills.columns)\n",
    "total_missing_cells = bills.isnull().sum().sum()\n",
    "completeness_score = (1 - total_missing_cells / total_possible_cells) * 100\n",
    "\n",
    "print(f\"Overall Data Completeness: {completeness_score:.1f}%\")\n",
    "print(f\"Missing cells: {total_missing_cells:,} out of {total_possible_cells:,}\")\n",
    "\n",
    "# Completeness by year\n",
    "if 'year' in bills.columns:\n",
    "    yearly_completeness = []\n",
    "    for year in sorted(bills['year'].unique()):\n",
    "        year_data = bills[bills['year'] == year]\n",
    "        year_missing = year_data.isnull().sum().sum()\n",
    "        year_total = len(year_data) * len(year_data.columns)\n",
    "        year_completeness = (1 - year_missing / year_total) * 100 if year_total > 0 else 0\n",
    "        yearly_completeness.append({\n",
    "            'year': year,\n",
    "            'records': len(year_data),\n",
    "            'completeness': year_completeness,\n",
    "            'missing_cells': year_missing\n",
    "        })\n",
    "    \n",
    "    completeness_df = pd.DataFrame(yearly_completeness)\n",
    "    \n",
    "    # Find years with low completeness\n",
    "    low_completeness_years = completeness_df[completeness_df['completeness'] < 95]\n",
    "    if len(low_completeness_years) > 0:\n",
    "        print(f\"\\nYears with <95% completeness:\")\n",
    "        for _, year_info in low_completeness_years.iterrows():\n",
    "            print(f\"  {year_info['year']}: {year_info['completeness']:.1f}% complete ({year_info['records']} records)\")\n",
    "    \n",
    "    # Visualize completeness over time\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "    fig.suptitle('Data Completeness Over Time', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Completeness percentage\n",
    "    axes[0].plot(completeness_df['year'], completeness_df['completeness'], \n",
    "                 marker='o', linewidth=2, color='green')\n",
    "    axes[0].set_ylabel('Completeness (%)')\n",
    "    axes[0].set_title('Data Completeness by Year')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].axhline(y=95, color='red', linestyle='--', alpha=0.7, label='95% threshold')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Number of records\n",
    "    axes[1].bar(completeness_df['year'], completeness_df['records'], \n",
    "                alpha=0.7, color='blue')\n",
    "    axes[1].set_xlabel('Year')\n",
    "    axes[1].set_ylabel('Number of Records')\n",
    "    axes[1].set_title('Data Volume by Year')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Completeness by parish\n",
    "if 'parish_id' in bills.columns:\n",
    "    parish_completeness = []\n",
    "    for parish_id in bills['parish_id'].unique():\n",
    "        parish_data = bills[bills['parish_id'] == parish_id]\n",
    "        parish_missing = parish_data.isnull().sum().sum()\n",
    "        parish_total = len(parish_data) * len(parish_data.columns)\n",
    "        parish_comp = (1 - parish_missing / parish_total) * 100 if parish_total > 0 else 0\n",
    "        \n",
    "        parish_name = parishes.set_index('id')['parish_name'].get(parish_id, f'Parish {parish_id}') if parishes is not None else f'Parish {parish_id}'\n",
    "        \n",
    "        parish_completeness.append({\n",
    "            'parish_id': parish_id,\n",
    "            'parish_name': parish_name,\n",
    "            'records': len(parish_data),\n",
    "            'completeness': parish_comp\n",
    "        })\n",
    "    \n",
    "    parish_comp_df = pd.DataFrame(parish_completeness)\n",
    "    \n",
    "    # Parishes with low completeness\n",
    "    low_comp_parishes = parish_comp_df[parish_comp_df['completeness'] < 95].sort_values('completeness')\n",
    "    if len(low_comp_parishes) > 0:\n",
    "        print(f\"\\nParishes with <95% completeness:\")\n",
    "        for _, parish_info in low_comp_parishes.head(10).iterrows():\n",
    "            print(f\"  {parish_info['parish_name'][:30]:30} - {parish_info['completeness']:.1f}% ({parish_info['records']} records)\")\n",
    "    \n",
    "    print(f\"\\nCompleteness Summary:\")\n",
    "    print(f\"  Average parish completeness: {parish_comp_df['completeness'].mean():.1f}%\")\n",
    "    print(f\"  Parishes with 100% completeness: {(parish_comp_df['completeness'] == 100).sum()}\")\n",
    "    print(f\"  Parishes with <90% completeness: {(parish_comp_df['completeness'] < 90).sum()}\")\n",
    "\n",
    "# Flag columns for specific quality issues\n",
    "if 'missing' in bills.columns or 'illegible' in bills.columns:\n",
    "    print(f\"\\nüö© Quality Flags in Data:\")\n",
    "    \n",
    "    if 'missing' in bills.columns:\n",
    "        missing_flagged = bills['missing'].sum()\n",
    "        print(f\"  Records flagged as 'missing': {missing_flagged:,} ({missing_flagged/len(bills)*100:.1f}%)\")\n",
    "    \n",
    "    if 'illegible' in bills.columns:\n",
    "        illegible_flagged = bills['illegible'].sum()\n",
    "        print(f\"  Records flagged as 'illegible': {illegible_flagged:,} ({illegible_flagged/len(bills)*100:.1f}%)\")\n",
    "    \n",
    "    # Combined quality issues\n",
    "    if 'missing' in bills.columns and 'illegible' in bills.columns:\n",
    "        quality_issues_total = bills['missing'].sum() + bills['illegible'].sum()\n",
    "        print(f\"  Total quality-flagged records: {quality_issues_total:,} ({quality_issues_total/len(bills)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations for Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üõ†Ô∏è Data Quality Recommendations:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Based on the analysis above, generate specific recommendations\n",
    "\n",
    "# 1. Missing data recommendations\n",
    "if missing_df['null_count'].sum() > 0:\n",
    "    high_missing_cols = missing_df[missing_df['null_percentage'] > 5]\n",
    "    if len(high_missing_cols) > 0:\n",
    "        recommendations.append({\n",
    "            'priority': 'HIGH',\n",
    "            'category': 'Missing Data',\n",
    "            'issue': f\"{len(high_missing_cols)} columns with >5% missing data\",\n",
    "            'recommendation': 'Investigate patterns, consider imputation or exclusion criteria'\n",
    "        })\n",
    "\n",
    "# 2. Consistency issues\n",
    "if consistency_issues:\n",
    "    recommendations.append({\n",
    "        'priority': 'HIGH',\n",
    "        'category': 'Data Consistency',\n",
    "        'issue': f\"{len(consistency_issues)} consistency issues detected\",\n",
    "        'recommendation': 'Review and correct data entry errors, validate against source documents'\n",
    "    })\n",
    "\n",
    "# 3. Outlier recommendations\n",
    "if 'count' in bills.columns:\n",
    "    outlier_count = len(bills[(bills['count'] > bills['count'].quantile(0.99) * 3)])\n",
    "    if outlier_count > 0:\n",
    "        recommendations.append({\n",
    "            'priority': 'MEDIUM',\n",
    "            'category': 'Statistical Outliers',\n",
    "            'issue': f\"{outlier_count} extreme outliers in death counts\",\n",
    "            'recommendation': 'Verify against historical records, check for transcription errors'\n",
    "        })\n",
    "\n",
    "# 4. Historical plausibility\n",
    "if plausibility_issues:\n",
    "    recommendations.append({\n",
    "        'priority': 'MEDIUM',\n",
    "        'category': 'Historical Plausibility',\n",
    "        'issue': f\"{len(plausibility_issues)} plausibility concerns\",\n",
    "        'recommendation': 'Cross-reference with historical sources and demographic data'\n",
    "    })\n",
    "\n",
    "# 5. Completeness recommendations\n",
    "if completeness_score < 95:\n",
    "    recommendations.append({\n",
    "        'priority': 'MEDIUM',\n",
    "        'category': 'Data Completeness',\n",
    "        'issue': f\"Overall completeness {completeness_score:.1f}% (<95%)\",\n",
    "        'recommendation': 'Assess impact of missing data on analysis, consider additional data sources'\n",
    "    })\n",
    "\n",
    "# 6. Duplicate data\n",
    "key_columns = ['parish_id', 'year', 'week_id', 'count_type']\n",
    "if all(col in bills.columns for col in key_columns):\n",
    "    duplicate_count = bills.duplicated(subset=key_columns).sum()\n",
    "    if duplicate_count > 0:\n",
    "        recommendations.append({\n",
    "            'priority': 'HIGH',\n",
    "            'category': 'Duplicate Records',\n",
    "            'issue': f\"{duplicate_count} potential duplicate records\",\n",
    "            'recommendation': 'Remove or consolidate duplicates after verification'\n",
    "        })\n",
    "\n",
    "# 7. Format standardization\n",
    "recommendations.append({\n",
    "    'priority': 'LOW',\n",
    "    'category': 'Standardization',\n",
    "    'issue': 'Data format consistency',\n",
    "    'recommendation': 'Standardize date formats, parish names, and categorical variables'\n",
    "})\n",
    "\n",
    "# Display recommendations\n",
    "if recommendations:\n",
    "    priority_order = {'HIGH': 1, 'MEDIUM': 2, 'LOW': 3}\n",
    "    recommendations.sort(key=lambda x: priority_order[x['priority']])\n",
    "    \n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        priority_emoji = {'HIGH': 'üî¥', 'MEDIUM': 'üü°', 'LOW': 'üü¢'}[rec['priority']]\n",
    "        print(f\"\\n{i}. {priority_emoji} {rec['priority']} - {rec['category']}\")\n",
    "        print(f\"   Issue: {rec['issue']}\")\n",
    "        print(f\"   Recommendation: {rec['recommendation']}\")\n",
    "else:\n",
    "    print(\"‚úÖ No major data quality issues requiring immediate attention\")\n",
    "\n",
    "# Summary quality score\n",
    "quality_factors = {\n",
    "    'completeness': min(completeness_score / 100, 1.0),\n",
    "    'consistency': 1.0 - min(len(consistency_issues) / 10, 1.0),  # Penalize up to 10 issues\n",
    "    'plausibility': 1.0 - min(len(plausibility_issues) / 10, 1.0),\n",
    "    'outliers': 1.0 - min((outlier_count if 'outlier_count' in locals() else 0) / len(bills), 0.1)  # Cap penalty at 10%\n",
    "}\n",
    "\n",
    "overall_quality_score = np.mean(list(quality_factors.values())) * 100\n",
    "\n",
    "print(f\"\\nüìä Overall Data Quality Score: {overall_quality_score:.1f}/100\")\n",
    "print(f\"\\nQuality Factor Breakdown:\")\n",
    "for factor, score in quality_factors.items():\n",
    "    print(f\"  {factor.title():15} - {score*100:5.1f}%\")\n",
    "\n",
    "print(f\"\\nüìã Data Quality Summary:\")\n",
    "if overall_quality_score >= 90:\n",
    "    print(f\"  ‚úÖ Excellent data quality - minimal issues detected\")\n",
    "elif overall_quality_score >= 75:\n",
    "    print(f\"  ‚úì Good data quality - some issues to address\")\n",
    "elif overall_quality_score >= 60:\n",
    "    print(f\"  ‚ö†Ô∏è Fair data quality - several issues need attention\")\n",
    "else:\n",
    "    print(f\"  üî¥ Poor data quality - significant issues require immediate attention\")\n",
    "\n",
    "print(f\"\\nüí° Next Steps:\")\n",
    "print(f\"  1. Address HIGH priority recommendations first\")\n",
    "print(f\"  2. Create data cleaning pipeline with validation rules\")\n",
    "print(f\"  3. Document all data transformations and exclusions\")\n",
    "print(f\"  4. Establish ongoing data quality monitoring\")\n",
    "print(f\"  5. Cross-validate against external historical sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### Data Quality Assessment Summary\n",
    "\n",
    "This comprehensive analysis evaluated the Bills of Mortality dataset across multiple dimensions:\n",
    "\n",
    "#### Key Findings:\n",
    "- **Missing Data**: Systematic assessment of null values and patterns\n",
    "- **Consistency**: Validation of data integrity and relationships\n",
    "- **Outliers**: Statistical identification of extreme values\n",
    "- **Historical Plausibility**: Cross-reference with known historical patterns\n",
    "- **Completeness**: Temporal and spatial coverage analysis\n",
    "\n",
    "#### Quality Score Methodology:\n",
    "- **Completeness**: Percentage of non-null values\n",
    "- **Consistency**: Absence of logical contradictions\n",
    "- **Plausibility**: Alignment with historical expectations\n",
    "- **Outliers**: Reasonable distribution of values\n",
    "\n",
    "### Recommended Data Cleaning Pipeline:\n",
    "\n",
    "1. **Validation Phase**:\n",
    "   - Verify extreme values against source documents\n",
    "   - Cross-check with external plague mortality records\n",
    "   - Validate parish names and geographical consistency\n",
    "\n",
    "2. **Cleaning Phase**:\n",
    "   - Remove confirmed duplicates\n",
    "   - Standardize categorical variables\n",
    "   - Apply missing data strategies (imputation vs. exclusion)\n",
    "\n",
    "3. **Enhancement Phase**:\n",
    "   - Add data quality flags to records\n",
    "   - Create confidence intervals for uncertain values\n",
    "   - Develop uncertainty propagation methods\n",
    "\n",
    "4. **Documentation Phase**:\n",
    "   - Record all transformations and decisions\n",
    "   - Create metadata for quality indicators\n",
    "   - Establish version control for cleaned datasets\n",
    "\n",
    "### Research Implications:\n",
    "\n",
    "- **High-quality periods**: Focus analysis on years/parishes with complete data\n",
    "- **Uncertainty quantification**: Include data quality in statistical models\n",
    "- **Comparative analysis**: Weight findings by data quality scores\n",
    "- **Historical validation**: Use quality assessment to guide source criticism\n",
    "\n",
    "### Ongoing Quality Assurance:\n",
    "\n",
    "- Regular re-assessment as new data is added\n",
    "- Automated quality checks in processing pipeline\n",
    "- Cross-validation with newly discovered sources\n",
    "- Community review and collaborative validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
